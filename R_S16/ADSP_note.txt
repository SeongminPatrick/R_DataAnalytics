----------------------------------------------------------------------------------------
###############################################################
#
# 정형 데이터 마이닝
#
###############################################################

Supervised Learning - 목표 변수 존재
	1) 분류 (Classification) : 목표 변수가 범주형
	2) 추정/예측 (Estimation) : 목표 변수가 연속형

Unsupervised Learning - 목표 변수가 없이 데이터 내부의 특성 이용
	1) 군집분석(Clustering)
	2) 연관규칙(Association rule)
	3) 연속규칙(Sequence rule)


--------------------------------------------------------------
분류분석 (Classification)
--------------------------------------------------------------

Supervised learning 기법
미리 정의된 그룹으로 데이터를 분류 (목표 변수가 범주형)
회귀분석 / 의사결정나무 / 베이지안 분류 / 인공신경망 / k Nearest Neighborhood


--------------------------------------------------------------
1. Decision Tree

(1) Decision Tree 활용

- 세분화/분류 : 데이터를 비슷한 특성을 갖는 몇 개의 그룹 또는 몇 개의 등급으로 분할
- 예측 : 데이터에서 규칙을 찾아낸 후 이를 이용하여 미래의 사건을 예측
- 차원 축소 및 변수 선택 : 목표 변수에 큰 영향을 미치는 변수를 골라냄
- 변수들 간의 교호작용 (interaction effect) 파악
- 목표 변수의 범주를 병합하거나 연속형 목표 변수를 이산화(binning)

(2) Decision Tree의 특징

- 결과를 설명하기 쉽다.
- 모형을 만드는 방법이 계산적으로 복잡하지 않다.
- 대용량 데이터도 빠르게 처리할 수 있다.
- 비정상 잡음 데이터에 대해서 민감하지 않다.
- 불필요한 변수가 있어도 크게 영향받지 않는다.

(3) Decision Tree 알고리즘

a. CART (Classification and Regression Tree)
	- 가장 많이 활용되는 알고리즘.
	- 목표 변수가 범주형, 연속형 모두 가능. 이진 분리.
	- 불순도 측도는 범주형일 경우 지니 지수, 연속형인 경우 분산을 사용.
	- 개별 입력변수 뿐만 아니라 입력변수들의 선형결합드 중에서 최적의 분리를 찾을 수 있다.
	
b. C4.5 & C5.0
	- 다지 분리(multiple split) 가능
	- 불순도 측도는 엔트로피 지수 사용.

c. CHAID (Chi-squared Automatic Interaction Detection)
	- 적당한 크기에서 나무모형의 성장을 중지
	- 불순도 측도는 카이제곱 통계량 사용.


--------------------------------------------------------------
# party package 이용한 Decision Tree
--------------------------------------------------------------

library(party)
library(caret)

# train / test data 분리 (6:4 or 7:3)
idx <- sample(2, nrow(iris), replace = T, prob = c(0.6, 0.4))
table(idx)
train_1 <- iris[idx == 1, ]
test_1 <- iris[idx == 2, ]

# train data 이용한 모델 
tree_model <- ctree(Species ~ ., data = train_1)
tree_model
plot(tree_model)
plot(tree_model, type = "simple")

# 예측된 데이터와 실제 데이터 비교
table(train_1$Species)                        # real data
train_1$pred <- predict(tree_model)
confusionMatrix(train_1$pred, train_1$Species)  # Accuracy : 0.9762

# Test Data로 검증
test_1$pred <- predict(tree_model, newdata = test_1)
confusionMatrix(test_1$pred, test_1$Species)    # Accuracy : 0.9394


--------------------------------------------------------------
# C50 패키지 이용한 Decision Tree
--------------------------------------------------------------

library(C50)

# train / test data 분리 (6:4)
idx <- sample(2, nrow(iris), replace = T, prob = c(0.6, 0.4))
table(idx)
train_2 <- iris[idx == 1, ]
test_2 <- iris[idx == 2, ]

# modeling
c5_options <- C5.0Control(winnow = FALSE, noGlobalPruning = FALSE)

c5_model <- C5.0(Species ~ ., data = train_2, control=c5_options, rules=FALSE)
summary(c5_model)
plot(c5_model)

# validation
train_2$pred <- predict(c5_model, newdata = train_2)
confusionMatrix(train_2$pred, train_2$Species)      # Accuracy : 0.9885

test_2$pred <- predict(c5_model, newdata = test_2)
confusionMatrix(test_2$pred, test_2$Species)        # Accuracy : 0.9048


----------------------------------------------------------------------------------------
이상치(outlier) 찾기와 처리


outlier 식별
- EDS (Extreme Studentized Deviation) : 평균에서 3 표준편차 이상 떨어진 값
- 사분위수 이용. boxplot outer fence 벗어난 값



----------------------------------------------------------------------------------------
결측값 처리

1. 단순 대치법 (Single Imputation)

- completes analysis : 결측값 존재하는 레코드 삭제
- mean imputaion : 관측 데이터의 평균으로 대치(비조건부) 또는 회귀분석을 활용한 대치(조건부)

2. 다중 대치법 (Multiple Imputation)

- 단순 대치법을 m 번 수행. imputation - analysis - combination step.
- Amelia : time series cross sectional dataset을 활용

3. imputation in R

- rfImpute() : Random Forest 모델은 결측값 존재시 바로 에러 발생. 이 함수 이용하여 결측값 대치 후 알고리즘 적용.
- complete.cases() : 데이터 내에 결측값 있으면 False
- is.na() : 결측값이 NA 인지 체크
- centralImputation() : DMwR 패키지. 중위수 또는 최빈값(factor)으로 대치
- knnImputation() : DMwR 패키지. knn 분류 알고리즘 사용
- amelia()


# Impute
library(Hmisc)

df <- data.frame(age = c(11, 23, NA, 40, 35, 15), gender = c('female', 'male'))
df

df$imputed_age <- with(df, impute(age, mean))
df$imputedR_age <- with(df, impute(age, "random"))
df


# Amelia
library(Amelia)
?amelia

trade <- freetrade
head(trade)     # tariff 관세 - NA's :58
summary(trade)

missmap(trade)  # 결측치 분포 시각화

# 결측치 대치값 생성. 시작값 = min(tariff)
am_data <- amelia(trade, m = 5, ts = "year", cs = "country", startvals =  7.10)
        # m	: the number of imputed datasets to create.
        # ts : time series column
        # cs : cross section variable

am_data
summary(am_data$imputations[[1]]$tariff)
summary(am_data$imputations[[2]]$tariff)
summary(am_data$imputations[[3]]$tariff)

# 최소값이 0보다 큰 첫번째 데이터 적용
impute1 <- am_data$imputations[[1]]$tariff
hist(impute1, col = "grey", border = "black")

trade$tariff <- impute1     # 결측치 대치
missmap(trade)

plot(am_data)
par(mfrow=c(1,1))


----------------------------------------------------------------------------------------
시계열 분석 Time Series


1. 시계열 자료 - 시간의 흐름에 따라 관찰된 데이터


2. 정상성 
대부분의 시계열 자료는 다루기 어려운 비정상성 시계열 자료이기 때문에 분석하기 쉬운 정상성 시계열 자료로 변환
(1) 평균이 일정 : 모든 시점에 대해 일정한 평균을 가진다. 
    - 평균이 일정하지 않은 시계열은 차분(difference)을 통해 정상화
    - 차분은 현시점 자료에서 이전 시점 자료를 빼는 것
(2) 분산도 시점에 의존하지 않음
    - 분산이 일정하지 않은 시계열은 변환(transformation)을 통해 정상화
(3) 공분산도 시차에만 의존할 뿐, 특정 시점에는 의존하지 않음


3. 시계열 모형

(1) 자기회귀 모형 (Autoregressive model, AR)

P 시점 이전의 자료가 현재 자료에 영향을 줌
오차항 = 백색잡음과정(white noise process)
자기상관함수(Autocorrelation Function, ACF) : k 기간 떨어진 값들의 상관계수
부분자기상관함수(partial ACF) : 서로 다른 두 시점의 중간에 있는 값들의 영향을 제외시킨 상관계수
ACF 빠르게 감소, PACF는 어느 시점에서 절단점을 갖는다
PACF가 2시점에서 절단점 가지면 AR(1) 모형

(2) 이동평균 모형 (Moving average model, MA)

유한한 갯수의 백색잡음 결합이므로 항상 정상성 만족
ACF가 절단점을 갖고, PACF는 빠르게 감소

(3) 자기회귀누적이동평균 모형 (Autoregressive integrated moving average model, ARIMA)

비정상 시계열 모형
차분이나 변환을 통해 AR, MA, 또는 이 둘을 합한 ARMA 모형으로 정상화
ARIMA(p, d, q) - d : 차분 차수 / p : AR 모형 차수 / q : MA 모형 차수

(4) 분해 시계열

시계열에 영향을 주는 일반적인 요인을 시계열에서 분리해 분석하는 방법
계절 요인(seasonal factor), 순환 요인(cyclical), 추세 요인(trend), 불규칙 요인(irregular)


----------------------------------------------------------------------------------------
다차원 척도법 (Multi-Dimensional Scaling, MDS)

개체들 사이의 유사성/비유사성을 측정하여 2차원 또는 3차원 공간상에 점으로 표현하는 분석 방법.
개체들간의 근접성(proximity)을 시각화하여 데이터 속에 잠재해 있는 패턴이나 구조를 찾아내는 통계 기법.
개체들간의 거리 계산은 유클리드 거리 행렬을 사용한다.
상대적 거리의 정확도를 높이기 위해 적합한 정도를 스트레스 값(stress value)으로 나타낸다.

1. 계량적 MDS

데이터가 연속형 변수(구간척도, 비율척도)인 경우 사용.
각 개체들간의 유클리드 거리 행렬을 계산하고 개체들간의 비유사성을 공간상에 표현한다.

library(MASS)
data(eurodist)              # 유럽 도시들간의 거리
eurodist

loc <- cmdscale(eurodist)   # 2차원으로 도시들을 mapping
loc

x <- loc[ , 1]
y <- -loc[ , 2]             # 북쪽 도시를 상단에 표시하기 위해 부호 변경

plot(x, y, type = "n", asp = 1, main = "Metric MDS")   # asp : y/x aspect ratio
text(x, y, rownames(loc), cex = 0.8)
abline(v = 0, h = 0, lty = 2, lwd = 1)


2. 비계량적 MDS

데이터가 순서척도인 경우 사용.
개체들간의 거리가 순서로 주어진 경우에는 순서척도를 거리의 속성과 같도록 변환하여 거리를 생성.

data(swiss)
head(swiss)     # 스위스 연방 주들의 사회경제적 지표

# (1) isoMDS : Kruskal's Non-metric Multidimensional Scaling

swissA <- as.matrix(swiss)
dist <- dist(swissA)        # make distance matrix
dist
mds <- isoMDS(dist)         # make points & stress
mds

plot(mds$points, type = "n")
text(mds$points, labels = rownames(swissA), cex = 0.7)
abline(v = 0, h = 0, lty = 2, lwd = 1)


swissC <- as.matrix(swiss[ , -2])    # Agriculture 제외하고 비교한 경우
distC <- dist(swissC)
mdsC <- isoMDS(distC)

plot(mdsC$points, type = "n")
text(mdsC$points, labels = rownames(swissC), cex = 0.8)
abline(v = 0, h = 0, lty = 2, lwd = 1)


# (2) sammon : Non-Linear Mapping

swissK <- as.matrix(swiss)
sam <- sammon(dist(swissK))

plot(sam$points, type = "n", main = "Nonmetric MDS : sammon")
text(sam$points, labels = rownames(swissK), cex = 0.7)
abline(v = 0, h = 0, lty = 2, lwd = 1)



----------------------------------------------------------------------------------------
주성분 분석 (Pricipal Component Analysis, PCA)

상관관계가 있는 변수들을 선형결합하여 변수를 축약하는 기법. 요인 분석의 한 종류. 
첫번째 주성분이 전체 변동을 가장 많이 설명하고, 두번째 주성분은 첫번째 주성분과 상관성이 낮아 
첫번째 주성분이 설명하지 못하는 나머지 변동을 정보의 손실없이 가장 많이 설명할 수 있도록 변수들을 조합.


1. 목적
- 여러 변수들 간에 내재하는 상관관계, 연관성을 이용해 소수의 주성분으로 차원을 축소
- 다중공선성이 존재하는 경우, 상관성이 적은 주성분으로 변수들을 축소하여 모형 개발에 활용
- 주성분분석을 통해 차원을 축소한 후 군집분석을 수행하면 결과와 연산속도를 개선할 수 있음
- 다량의 센서 데이터를 주성분분석으로 차원 축소한 후 시계열로 분포나 추세의 변화를 분석하여 고장 징후를 사전에 파악


2. 주성분 선택법

주성분분석 결과에서 누적기여율(cumulative proportion)이 85%이상인 주성분까지 선택
screen plot에서 고유값이 수평을 유지하기 전단계로 주성분의 수를 선택

df <- USArrests
library(psych)
pairs.panels(df)

pcomp <- princomp(df, cor = T)
summary(pcomp)
        # 제2 주성분까지의 cumulative proportion이 86.74% 이므로 
        # 2개의 주성분 변수를 활용하여 전체 데이터의 86.75%를 설명할 수 있다.

screeplot(pcomp, npcs = 4, type = "lines", main = "USArrests princomp")

# 변수들이 각 주성분에 기여하는 가중치 표시
pcomp$loadings
        # 제1 주성분에는 4개의 변수가 평균적으로 기여한다.
        # 제2 주성분에서는 (Murder, Assault)와 (UrbanPop, Rape) 계수의 부호가 다르다.

head(pcomp$scores);tail(pcomp$scores)

# 2개 주성분에 의한 plot
biplot(pcomp)
        # 알라스카, 루이지애나는 살인 비율이 높다
        # 미시건, 텍사스는 강간 비율이 높다.
        # 아이다호, 뉴햄프셔, 아이오와는 인구 비율이 낮으면서 강력범죄도 낮다.
















